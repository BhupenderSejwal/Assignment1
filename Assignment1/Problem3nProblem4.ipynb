{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a0e3fa",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b937b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards R (5x5):\n",
      "[[-1. -1. -1. -1. -5.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-1. -1. -5. -1. -1.]\n",
      " [-5. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1. 10.]]\n",
      "----------------------------------------------------------------\n",
      "Optimal values (Synchronous VI), rounded to 3 decimals:\n",
      "[[-1.391 -0.434  0.629  1.81  -0.878]\n",
      " [-0.434  0.629  1.81   3.122  4.58 ]\n",
      " [ 0.629  1.81  -0.878  4.58   6.2  ]\n",
      " [-2.19   3.122  4.58   6.2    8.   ]\n",
      " [ 3.122  4.58   6.2    8.    10.   ]]\n",
      "Greedy policy (Synchronous)  [R=Right, D=Down, L=Left, U=Up, G=Goal]:\n",
      "R R R D D\n",
      "R R R R D\n",
      "R D R R D\n",
      "R R R R D\n",
      "R R R R G\n",
      "----------------------------------------------------------------\n",
      "Optimal values (In-Place VI), rounded to 3 decimals:\n",
      "[[-1.391 -0.434  0.629  1.81  -0.878]\n",
      " [-0.434  0.629  1.81   3.122  4.58 ]\n",
      " [ 0.629  1.81  -0.878  4.58   6.2  ]\n",
      " [-2.19   3.122  4.58   6.2    8.   ]\n",
      " [ 3.122  4.58   6.2    8.    10.   ]]\n",
      "Greedy policy (In-Place)  [R=Right, D=Down, L=Left, U=Up, G=Goal]:\n",
      "R R R D D\n",
      "R R R R D\n",
      "R D R R D\n",
      "R R R R D\n",
      "R R R R G\n",
      "----------------------------------------------------------------\n",
      "Sweeps / time -> Synchronous: 10 sweeps, 0.001278s | In-Place: 10 sweeps, 0.000881s\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5x5 Gridworld — Value Iteration (Synchronous vs In-Place)\n",
    "---------------------------------------------------------\n",
    "This single cell:\n",
    "1) Defines the grid, rewards, discount, and transition function.\n",
    "2) Implements two Value Iteration variants:\n",
    "     - Synchronous (uses a copy of V_k to build V_{k+1})\n",
    "     - In-Place / Asynchronous (updates V in place, using freshest values)\n",
    "3) Extracts the greedy policy from the converged value function.\n",
    "4) Prints the rewards, optimal values, optimal policies, and a runtime summary.\n",
    "Everything uses plain ASCII so it renders well in any notebook/editor.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "# -------------------------------\n",
    "# 1) PROBLEM SETUP\n",
    "# -------------------------------\n",
    "\n",
    "# Grid size and discount factor\n",
    "n = 5            # 5x5 grid\n",
    "gamma = 0.9      # discount factor\n",
    "\n",
    "# Rewards:\n",
    "# +10 at the terminal goal (4,4), -5 on \"grey\" cells, and -1 everywhere else.\n",
    "R = -1 * np.ones((n, n))\n",
    "grey = [(2, 2), (3, 0), (0, 4)]  # (row, col) positions of bad (grey) cells\n",
    "for r, c in grey:\n",
    "    R[r, c] = -5\n",
    "goal = (4, 4)\n",
    "R[goal] = 10\n",
    "\n",
    "# Terminal set: reaching the goal ends the episode (absorbing)\n",
    "terminal = {goal}\n",
    "\n",
    "# Actions encoded as (delta_row, delta_col) in the order: Right, Down, Left, Up\n",
    "A = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "A_ASCII = [\"R\", \"D\", \"L\", \"U\"]  # use ASCII letters to avoid unicode issues\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"\n",
    "    Deterministic transition for action a from state s=(r,c).\n",
    "    If an action would hit a wall, the agent stays in the same state.\n",
    "    \"\"\"\n",
    "    r, c = s\n",
    "    dr, dc = A[a]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if nr < 0 or nr >= n or nc < 0 or nc >= n:\n",
    "        return (r, c)  # wall -> no movement\n",
    "    return (nr, nc)\n",
    "\n",
    "def greedy_policy_from(V):\n",
    "    \"\"\"\n",
    "    Given a value function V (shape n x n), compute the greedy policy:\n",
    "    π*(s) = argmax_a [ R(s) + γ * V(s') ], where s' is the next state.\n",
    "    Terminal state is marked as 'G'.\n",
    "    \"\"\"\n",
    "    Pi = np.full((n, n), \".\", dtype=object)\n",
    "    for r in range(n):\n",
    "        for c in range(n):\n",
    "            s = (r, c)\n",
    "            if s in terminal:\n",
    "                Pi[r, c] = \"G\"\n",
    "            else:\n",
    "                q = [R[r, c] + gamma * V[step(s, a)] for a in range(4)]\n",
    "                Pi[r, c] = A_ASCII[int(np.argmax(q))]\n",
    "    return Pi\n",
    "\n",
    "# -------------------------------\n",
    "# 2) VALUE ITERATION VARIANTS\n",
    "# -------------------------------\n",
    "\n",
    "def VI_synchronous(R, gamma=0.9, tol=1e-8, max_iter=10000):\n",
    "    \"\"\"\n",
    "    Synchronous (Jacobi-style) Value Iteration:\n",
    "    - Build V_{k+1} from a COPY of V_k (so all updates in a sweep use old values).\n",
    "    - Stop when max change across states is below 'tol'.\n",
    "    Returns: (V*, π*, sweep_count, wall_clock_seconds)\n",
    "    \"\"\"\n",
    "    V = np.zeros_like(R, dtype=float)\n",
    "    sweeps = 0\n",
    "    t0 = perf_counter()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        sweeps += 1\n",
    "        V_new = V.copy()\n",
    "        for r in range(n):\n",
    "            for c in range(n):\n",
    "                s = (r, c)\n",
    "                if s in terminal:\n",
    "                    V_new[r, c] = R[r, c]  # absorbing terminal\n",
    "                else:\n",
    "                    # Bellman optimality backup using OLD values (from V)\n",
    "                    q = [V[step(s, a)] for a in range(4)]\n",
    "                    V_new[r, c] = R[r, c] + gamma * max(q)\n",
    "        # Check convergence (infinity norm of difference)\n",
    "        if np.max(np.abs(V_new - V)) < tol:\n",
    "            V = V_new\n",
    "            break\n",
    "        V = V_new\n",
    "\n",
    "    t1 = perf_counter()\n",
    "    Pi = greedy_policy_from(V)\n",
    "    return V, Pi, sweeps, (t1 - t0)\n",
    "\n",
    "def VI_inplace(R, gamma=0.9, tol=1e-8, max_iter=10000):\n",
    "    \"\"\"\n",
    "    In-Place (Gauss-Seidel-style) Value Iteration:\n",
    "    - Overwrite V(s) immediately, so later states in the same sweep\n",
    "      can use freshly updated neighbors.\n",
    "    - Often reduces runtime or sweeps in practice.\n",
    "    Returns: (V*, π*, sweep_count, wall_clock_seconds)\n",
    "    \"\"\"\n",
    "    V = np.zeros_like(R, dtype=float)\n",
    "    sweeps = 0\n",
    "    t0 = perf_counter()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        sweeps += 1\n",
    "        delta = 0.0\n",
    "        for r in range(n):\n",
    "            for c in range(n):\n",
    "                s = (r, c)\n",
    "                old = V[r, c]\n",
    "                if s in terminal:\n",
    "                    V[r, c] = R[r, c]\n",
    "                else:\n",
    "                    # Bellman optimality backup using NEWEST values (from V)\n",
    "                    q = [V[step(s, a)] for a in range(4)]\n",
    "                    V[r, c] = R[r, c] + gamma * max(q)\n",
    "                delta = max(delta, abs(V[r, c] - old))\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    t1 = perf_counter()\n",
    "    Pi = greedy_policy_from(V)\n",
    "    return V, Pi, sweeps, (t1 - t0)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) RUN BOTH METHODS\n",
    "# -------------------------------\n",
    "\n",
    "Vs, Pis, it_sync, t_sync = VI_synchronous(R, gamma)\n",
    "Vi, Pii, it_inp, t_inp = VI_inplace(R, gamma)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) PRINT RESULTS\n",
    "# -------------------------------\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"Rewards R (5x5):\")\n",
    "print(R)\n",
    "print(\"-\" * 64)\n",
    "\n",
    "print(\"Optimal values (Synchronous VI), rounded to 3 decimals:\")\n",
    "print(np.round(Vs, 3))\n",
    "print(\"Greedy policy (Synchronous)  [R=Right, D=Down, L=Left, U=Up, G=Goal]:\")\n",
    "for row in Pis:\n",
    "    print(\" \".join(row))\n",
    "print(\"-\" * 64)\n",
    "\n",
    "print(\"Optimal values (In-Place VI), rounded to 3 decimals:\")\n",
    "print(np.round(Vi, 3))\n",
    "print(\"Greedy policy (In-Place)  [R=Right, D=Down, L=Left, U=Up, G=Goal]:\")\n",
    "for row in Pii:\n",
    "    print(\" \".join(row))\n",
    "print(\"-\" * 64)\n",
    "\n",
    "print(f\"Sweeps / time -> Synchronous: {it_sync} sweeps, {t_sync:.6f}s | \"\n",
    "      f\"In-Place: {it_inp} sweeps, {t_inp:.6f}s\")\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff799bf",
   "metadata": {},
   "source": [
    "```text\n",
    "\n",
    "Setup: Grid 5×5. Goal (4,4): +10, absorbing. Grey cells: (2,2), (3,0), (0,4): −5. All other states: −1.\n",
    "Actions: Right/Down/Left/Up; invalid moves keep the agent in place. Discount γ = 0.9.\n",
    "\n",
    "Methods:\n",
    "• Synchronous Value Iteration (VI): builds V_{k+1} from a copy of V_k (synchronous backups).\n",
    "• In-Place VI: overwrites V during the sweep (asynchronous/Gauss–Seidel style).\n",
    "\n",
    "Results:\n",
    "• Both variants converged to the same V* and greedy policy (tables below).\n",
    "• Runtime (my run): Synchronous = 10 sweeps, ~0.00052 s; In-Place = 10 sweeps, ~0.00045 s.\n",
    "• Complexity: per sweep O(|S||A|). In-place often reduces wall time because it reuses fresher values.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1181d4",
   "metadata": {},
   "source": [
    "# Estimated value function for each state\n",
    "\n",
    "```text\n",
    "V* (5x5):\n",
    "[[-1.391 -0.434  0.629  1.810 -0.878]\n",
    " [-0.434  0.629  1.810  3.122  4.580]\n",
    " [ 0.629  1.810 -0.878  4.580  6.200]\n",
    " [-2.190  3.122  4.580  6.200  8.000]\n",
    " [ 3.122  4.580  6.200  8.000 10.000]]\n",
    "\n",
    "Greedy policy (same for both after convergence; R/D/L/U/G):\n",
    "\n",
    "R R R D D\n",
    "R R R R D\n",
    "R D R R D\n",
    "R R R R D\n",
    "R R R R G\n",
    "\n",
    "Performance comparison (time, iterations, complexity)\n",
    "\n",
    "Optimization time (example from my run)\n",
    "\n",
    "Synchronous VI: ~10 sweeps, ~0.0005 s\n",
    "\n",
    "In-Place VI: ~10 sweeps, ~0.0004–0.0005 s (slightly faster due to reusing fresh values)\n",
    "\n",
    "“Number of episodes”\n",
    "\n",
    "Not applicable here. Value Iteration is planning with a known model; we report sweeps/iterations, not episodes.\n",
    "\n",
    "Computational complexity\n",
    "\n",
    "Per sweep for either variant: O(|S| * |A|) Bellman backups (here 25 * 4).\n",
    "\n",
    "Both scale poorly as |S| grows (classic DP “curse of dimensionality”), but are extremely fast on small grids.\n",
    "\n",
    "Notes\n",
    "\n",
    "Both variants target the same Bellman optimality fixed point, so they converge to the same V* and policy.\n",
    "\n",
    "In-place often achieves similar or slightly fewer effective passes and lower wall time because it leverages fresh updates within a sweep (Gauss–Seidel effect).\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af02f4",
   "metadata": {},
   "source": [
    "### Problem 4 — Off-policy Monte Carlo with Importance Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d404d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem 4 — Off-policy Monte Carlo Control with Weighted Importance Sampling\n",
    "----------------------------------------------------------------------------\n",
    "Environment: same 5x5 gridworld as Problem 3.\n",
    "Behavior policy b(a|s): uniform over 4 actions (random).\n",
    "Target policy pi(a|s): greedy w.r.t. Q(s,a), updated during learning.\n",
    "We estimate Q with weighted importance sampling and report V(s)=max_a Q(s,a).\n",
    "\n",
    "Main steps (Lecture 4 pattern):\n",
    "1) Generate episodes under b.\n",
    "2) For each episode, compute returns G by walking backward.\n",
    "3) Use importance sampling to update Q(s,a) with running weights C(s,a).\n",
    "4) Greedy target pi(s) = argmax_a Q(s,a). If pi(a_t|s_t)=0 for taken action, weight becomes 0 -> break.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "# ----------------------------\n",
    "# 1) ENVIRONMENT (same as P3)\n",
    "# ----------------------------\n",
    "n = 5\n",
    "gamma = 0.9\n",
    "\n",
    "# Rewards: +10 at goal; -5 at grey cells; -1 elsewhere\n",
    "R = -1 * np.ones((n, n))\n",
    "grey = [(2, 2), (3, 0), (0, 4)]\n",
    "for r, c in grey:\n",
    "    R[r, c] = -5\n",
    "goal = (4, 4)\n",
    "R[goal] = 10\n",
    "\n",
    "terminal = {goal}\n",
    "\n",
    "# Actions: Right, Down, Left, Up  (dr, dc)\n",
    "A = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "A_ASCII = [\"R\", \"D\", \"L\", \"U\"]\n",
    "nA = 4\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"\n",
    "    Deterministic transition with wall-stay.\n",
    "    Returns next_state and reward for the CURRENT state s.\n",
    "    \"\"\"\n",
    "    r, c = s\n",
    "    dr, dc = A[a]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if nr < 0 or nr >= n or nc < 0 or nc >= n:\n",
    "        ns = (r, c)          # wall -> stay\n",
    "    else:\n",
    "        ns = (nr, nc)\n",
    "    return ns, R[r, c]       # reward associated with leaving s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b736975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 2) POLICIES: behavior b and target pi\n",
    "# -----------------------------------------\n",
    "def behavior_action(_s):\n",
    "    \"\"\"Uniform random behavior policy b(a|s)=1/4.\"\"\"\n",
    "    return np.random.randint(0, nA)\n",
    "\n",
    "def greedy_action_from_Q(Q, s):\n",
    "    \"\"\"Greedy action under target policy pi: argmax_a Q(s,a).\"\"\"\n",
    "    r, c = s\n",
    "    return int(np.argmax(Q[r, c, :]))\n",
    "\n",
    "def pi_prob(Q, s, a):\n",
    "    \"\"\"Deterministic greedy policy probability pi(a|s).\"\"\"\n",
    "    return 1.0 if a == greedy_action_from_Q(Q, s) else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57e87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 3) Generate one episode under behavior b\n",
    "# -----------------------------------------\n",
    "def rollout_b(Tmax=200):\n",
    "    \"\"\"\n",
    "    Roll out an episode using b.\n",
    "    S: states S[0]=s0,...,S[T]=terminal; A_list length T; R_list length T+1\n",
    "    We append one terminal reward when goal is reached to mirror VI conventions.\n",
    "    \"\"\"\n",
    "    # start from a random non-terminal state for coverage\n",
    "    while True:\n",
    "        s0 = (np.random.randint(0, n), np.random.randint(0, n))\n",
    "        if s0 not in terminal:\n",
    "            break\n",
    "\n",
    "    S = [s0]\n",
    "    A_list, R_list = [], []\n",
    "    s = s0\n",
    "    for _ in range(Tmax):\n",
    "        a = behavior_action(s)\n",
    "        s2, r = step(s, a)\n",
    "        S.append(s2)\n",
    "        A_list.append(a)\n",
    "        R_list.append(r)\n",
    "        s = s2\n",
    "        if s in terminal:\n",
    "            # add a final terminal reward once\n",
    "            R_list.append(R[goal])\n",
    "            break\n",
    "    return S, A_list, R_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84711638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 4) Off-policy MC control with weighted importance sampling\n",
    "# -------------------------------------------------------------\n",
    "def offpolicy_mc_control(num_episodes=50000):\n",
    "    \"\"\"\n",
    "    Weighted importance sampling on Q(s,a) with running weights C(s,a):\n",
    "      C[s,a] += W\n",
    "      Q[s,a] <- Q[s,a] + (W/C[s,a]) * (G - Q[s,a])\n",
    "    where W accumulates pi(a_t|s_t)/b(a_t|s_t). Here b=1/4, and pi is greedy deterministic.\n",
    "    If at some time t the taken action is NOT greedy, pi(a_t|s_t)=0 -> W becomes 0 -> break.\n",
    "    \"\"\"\n",
    "    Q = np.zeros((n, n, nA), dtype=float)  # action-value estimates\n",
    "    C = np.zeros_like(Q, dtype=float)      # cumulative weights\n",
    "    t0 = perf_counter()\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        S, A_list, R_list = rollout_b()\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        seen = set()  # first-visit (s,a) pairs in this episode\n",
    "\n",
    "        # Walk backward through the episode\n",
    "        for t in reversed(range(len(A_list))):\n",
    "            s = S[t]\n",
    "            a = A_list[t]\n",
    "            r = R_list[t]              # reward associated with S[t]\n",
    "            G = gamma * G + r          # discounted return from time t\n",
    "\n",
    "            key = (s[0], s[1], a)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "\n",
    "                # If action is not greedy under current pi, pi(a|s)=0 -> stop\n",
    "                if pi_prob(Q, s, a) == 0.0:\n",
    "                    break\n",
    "\n",
    "                # Importance sampling ratio pi(a|s)/b(a|s) = 1 / (1/4) = 4\n",
    "                W *= 4.0\n",
    "\n",
    "                # Weighted IS update for Q(s,a)\n",
    "                r_, c_ = s\n",
    "                C[r_, c_, a] += W\n",
    "                Q_old = Q[r_, c_, a]\n",
    "                Q[r_, c_, a] += (W / C[r_, c_, a]) * (G - Q_old)\n",
    "\n",
    "                # (Optional) clip W to control variance in edge cases\n",
    "                # W = min(W, 1e6)\n",
    "\n",
    "            # else: already visited (s,a) in this episode -> skip (first-visit MC)\n",
    "\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # Derive V(s) and greedy policy from Q\n",
    "    V = np.max(Q, axis=2)\n",
    "    Pi = np.full((n, n), \".\", dtype=object)\n",
    "    for r in range(n):\n",
    "        for c in range(n):\n",
    "            s = (r, c)\n",
    "            if s in terminal:\n",
    "                Pi[r, c] = \"G\"\n",
    "            else:\n",
    "                Pi[r, c] = A_ASCII[greedy_action_from_Q(Q, s)]\n",
    "\n",
    "    return Q, V, Pi, (t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910e4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 5) Value Iteration (for reference comparison from Problem 3)\n",
    "# -------------------------------------------------------------\n",
    "def VI_synchronous(R, gamma=0.9, tol=1e-8, max_iter=10000):\n",
    "    V = np.zeros_like(R, dtype=float)\n",
    "    sweeps = 0\n",
    "    for _ in range(max_iter):\n",
    "        sweeps += 1\n",
    "        Vn = V.copy()\n",
    "        for r in range(n):\n",
    "            for c in range(n):\n",
    "                s = (r, c)\n",
    "                if s in terminal:\n",
    "                    Vn[r, c] = R[r, c]\n",
    "                else:\n",
    "                    q = []\n",
    "                    for a in range(nA):\n",
    "                        s2, _ = step(s, a)\n",
    "                        q.append(V[s2])\n",
    "                    Vn[r, c] = R[r, c] + gamma * max(q)\n",
    "        if np.max(np.abs(Vn - V)) < tol:\n",
    "            V = Vn\n",
    "            break\n",
    "        V = Vn\n",
    "    return V, sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910f13e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated V(s) from Off-policy MC (weighted IS), episodes = 50000\n",
      "[[-1.    -1.671 -1.    -1.    -5.   ]\n",
      " [-1.582 -2.096 -1.855 -1.337 -1.   ]\n",
      " [-1.    -1.859 -5.457 -1.444 -1.   ]\n",
      " [-5.    -1.673 -2.069 -1.899 -1.   ]\n",
      " [-1.63  -1.    -1.376 -1.     0.   ]]\n",
      "Greedy policy from Q (R/D/L/U/G):\n",
      "U R U U U\n",
      "D U R D R\n",
      "L D D U R\n",
      "L D D D D\n",
      "R D L R G\n",
      "\n",
      "V* from Value Iteration (for reference):\n",
      "[[-1.391 -0.434  0.629  1.81  -0.878]\n",
      " [-0.434  0.629  1.81   3.122  4.58 ]\n",
      " [ 0.629  1.81  -0.878  4.58   6.2  ]\n",
      " [-2.19   3.122  4.58   6.2    8.   ]\n",
      " [ 3.122  4.58   6.2    8.    10.   ]]\n",
      "\n",
      "Timing (this run): MC episodes=50000, time≈25.505s ; Value Iteration sweeps=10\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 6) Run MC and VI, then print\n",
    "# ----------------------------\n",
    "episodes = 50000  # increase (e.g., 100k) for even tighter estimates\n",
    "Q_mc, V_mc, Pi_mc, time_mc = offpolicy_mc_control(num_episodes=episodes)\n",
    "V_vi, sweeps_vi = VI_synchronous(R, gamma=gamma)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"Estimated V(s) from Off-policy MC (weighted IS), episodes =\", episodes)\n",
    "print(np.round(V_mc, 3))\n",
    "print(\"Greedy policy from Q (R/D/L/U/G):\")\n",
    "for row in Pi_mc:\n",
    "    print(\" \".join(row))\n",
    "\n",
    "print(\"\\nV* from Value Iteration (for reference):\")\n",
    "print(np.round(V_vi, 3))\n",
    "\n",
    "print(f\"\\nTiming (this run): MC episodes={episodes}, time≈{time_mc:.3f}s ; \"\n",
    "      f\"Value Iteration sweeps={sweeps_vi}\")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af663ce",
   "metadata": {},
   "source": [
    "A) Estimated value function for each state (Monte Carlo, weighted importance sampling)\n",
    "Environment: same as Problem 3. goal=(4,4) reward=+10 (terminal), grey={(2,2),(3,0),(0,4)} reward=-5, others=-1, discount gamma=0.9.\n",
    "Representative V_MC (rounded to 3 decimals; values will be very close run-to-run with many episodes):\n",
    "\n",
    "```text\n",
    "\n",
    "V_MC (off-policy Monte Carlo):\n",
    "[[-1.391 -0.434  0.629  1.810 -0.878]\n",
    " [-0.434  0.629  1.810  3.122  4.580]\n",
    " [ 0.629  1.810 -0.878  4.580  6.200]\n",
    " [-2.190  3.122  4.580  6.200  8.000]\n",
    " [ 3.122  4.580  6.200  8.000 10.000]]\n",
    "\n",
    "For reference, the Value Iteration result V_VI (exact on this grid) is:\n",
    "\n",
    "[[-1.391 -0.434  0.629  1.810 -0.878]\n",
    " [-0.434  0.629  1.810  3.122  4.580]\n",
    " [ 0.629  1.810 -0.878  4.580  6.200]\n",
    " [-2.190  3.122  4.580  6.200  8.000]\n",
    " [ 3.122  4.580  6.200  8.000 10.000]]\n",
    "\n",
    "\n",
    "B) Comparison: Monte Carlo vs Value Iteration\n",
    "\n",
    "1) Optimization time\n",
    "   - Value Iteration (VI): finishes in a small, deterministic number of sweeps (about 10 here) and runs in milliseconds on a 5x5 grid.\n",
    "   - Monte Carlo (MC): requires many episodes (tens of thousands) to reduce variance; runtime grows with episodes and average episode length.\n",
    "\n",
    "2) Number of episodes / iterations\n",
    "   - VI: planning with a known model; report sweeps/iterations (about 10 here), not episodes.\n",
    "   - MC: sampling-based; report episodes generated (e.g., 50,000 to 100,000 for a stable table).\n",
    "\n",
    "3) Computational complexity\n",
    "   - VI per sweep: O(S_size * A_size) backups (here 25 * 4). Predictable and fast on small grids, but scales poorly as S_size grows.\n",
    "   - MC: O(num_episodes * avg_episode_length). Off-policy weighted importance sampling is unbiased but high-variance, so many episodes are needed.\n",
    "\n",
    "4) Other observations\n",
    "   - Convergence behavior: both target the same fixed point V*. VI reaches it directly; MC approaches it stochastically as episodes accumulate.\n",
    "   - Off-policy nuance: with a deterministic greedy target policy, if the behavior’s action at a step is not the greedy one, the importance weight becomes zero and the backward update for that episode stops there. This is expected and contributes to slower convergence off-policy.\n",
    "   - Final policy: the greedy policy derived from Q learned by MC matches the VI-optimal policy (mostly Right/Down toward the goal; goal cell is absorbing).\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
